defaults:
  - /preset/ppo/walker_walk@_here_
  - /preset/pref/walker_walk@_here_
  - _self_

method:
  _target_: pref_rl.methods.pref_ppo.PrefPPO
  save_callback_data: false
  save_episode_data: false

  unsuper:
    n_steps_unsuper: 32_000
    n_epochs_unsuper: 50

  pref:
    n_epochs_reward: 100
    train_acc_threshold_reward: 0.97
    learning_rate_reward: 3.e-4
    batch_size_reward: 128
    reward_model_kwargs:
      net_arch: [256, 256, 256]
      ensemble_size: 3
      activation_fn:
        _target_: torch.nn.LeakyReLU
      output_fn:
        _target_: torch.nn.Tanh
