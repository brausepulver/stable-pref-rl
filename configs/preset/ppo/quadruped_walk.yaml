defaults:
  - /envs/quadruped_walk@env
  - _self_

method:
  _target_: stable_baselines3.PPO
  device: cpu

  learning_rate: 5.0e-5
  batch_size: 128
  ent_coef: 0
  n_steps: 500
  gae_lambda: 0.9
  n_epochs: 20
  use_sde: true
  sde_sample_freq: 4

  policy_kwargs:
    net_arch:
      pi: [256, 256, 256]
      vf: [256, 256, 256]

  clip_range:
    _target_: pref_rl.utils.schedules.LinearSchedule
    start: 0.4
    end: 0.3

logging:
  log_freq_rollouts: 2
