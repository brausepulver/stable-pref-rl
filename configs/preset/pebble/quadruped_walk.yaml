defaults:
  - /preset/sac/quadruped_walk@_here_
  - /preset/pref/quadruped_walk@_here_
  - _self_

method:
  _target_: pref_rl.methods.pebble.PEBBLE
  learning_starts: 1000
  n_steps_seed: 1000
  reset_gradient_steps: 100

  unsuper:
    n_steps_unsuper: 9_000

  pref:
    sampler: disagreement

    n_epochs_reward: 50
    train_acc_threshold_reward: 0.97
    learning_rate_reward: 3.e-4
    batch_size_reward: 128
    reward_model_kwargs:
      net_arch: [256, 256, 256]
      ensemble_size: 3
      activation_fn:
        _target_: torch.nn.LeakyReLU
      output_fn:
        _target_: torch.nn.Tanh

    schedule:
      n_steps_reward: 30_000
